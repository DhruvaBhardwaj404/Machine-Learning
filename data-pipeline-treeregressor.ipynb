{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ntrain=pd.read_csv('/kaggle/input/titanic/train.csv')\ntest=pd.read_csv('/kaggle/input/titanic/test.csv')\n\ncolumns=trainData.columns\n#print(columns)\nTrain_X=trainData[['Pclass', 'Sex', 'Age']]\nTrain_Y=trainData['Survived']\n#print(Train_X.head())\nprint(Train_Y.head())\n\n","metadata":{"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"0    0\n1    1\n2    1\n3    1\n4    0\nName: Survived, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x,test_x,train_y,test_y=train_test_split(Train_X,Train_Y)\n#print(train_y)\nmyImputer=SimpleImputer(strategy='mean')\nimputed_train_X = pd.DataFrame(myImputer.fit_transform(train_x.drop('Sex',axis=1)))\nimputed_test_X = pd.DataFrame(myImputer.fit_transform(test_x.drop('Sex',axis=1)))\n\nimputed_train_X.columns=['Pclass', 'Age']\nimputed_test_X.columns=['Pclass', 'Age']\n\n#print(imputed_train_X.columns)","metadata":{"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder= LabelEncoder()\nencoded_train_X= pd.DataFrame(train_x['Sex'])\nencoded_test_X= pd.DataFrame(test_x['Sex'])\n#print(encoded_train_X.head())\nenc_train_X= pd.DataFrame(encoder.fit_transform(encoded_train_X['Sex']))\nenc_test_X= pd.DataFrame(encoder.fit_transform(encoded_test_X['Sex']))\nenc_train_X.columns=['Sex']\nenc_test_X.columns=['Sex']\n#print(enc_train_X.head())\n#print(imputed_train_X.head())\nfinal_X=pd.concat([imputed_train_X,enc_train_X],axis=1)\nfinal_test=pd.concat([imputed_test_X,enc_test_X],axis=1)\n#final_X.columns=['Pclass', 'Sex', 'Age']\n#print(final_X.columns)","metadata":{"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodel=RandomForestRegressor()\n#print(train_y)\nmodel.fit(final_X,train_y)\npredictions=model.predict(final_test)\nrms=mean_squared_error(predictions,test_y)\nprint(rms)","metadata":{"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"0.17000736630555474\n","output_type":"stream"}]}]}